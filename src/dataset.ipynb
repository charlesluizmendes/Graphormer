{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75516570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import os\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8b263c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerador de Dataset Balanceado para Roteamento - Versão Notebook\n",
    "# Execute cada célula em ordem para gerar seus datasets\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import os\n",
    "from tqdm.auto import tqdm  # Progress bars bonitas no notebook\n",
    "\n",
    "class RoutingDatasetGenerator:\n",
    "    def __init__(self, seed=42):\n",
    "        \"\"\"\n",
    "        Gerador de dataset balanceado para problemas de roteamento\n",
    "        \"\"\"\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        print(f\"Seed definida como: {seed}\")\n",
    "        \n",
    "    def generate_graph(self, num_nodes: int, edge_prob: float = 0.3, \n",
    "                      min_edges_per_node: int = 2) -> nx.Graph:\n",
    "        \"\"\"\n",
    "        Gera um grafo conectado com características de rede realistas\n",
    "        \"\"\"\n",
    "        # Começar com um grafo aleatório\n",
    "        G = nx.erdos_renyi_graph(num_nodes, edge_prob)\n",
    "        \n",
    "        # Garantir conectividade\n",
    "        while not nx.is_connected(G):\n",
    "            components = list(nx.connected_components(G))\n",
    "            if len(components) > 1:\n",
    "                node1 = random.choice(list(components[0]))\n",
    "                node2 = random.choice(list(components[1]))\n",
    "                G.add_edge(node1, node2)\n",
    "        \n",
    "        # Garantir grau mínimo para todos os nós\n",
    "        for node in G.nodes():\n",
    "            if G.degree(node) < min_edges_per_node:\n",
    "                available_nodes = [n for n in G.nodes() if n != node and not G.has_edge(node, n)]\n",
    "                if available_nodes:\n",
    "                    targets_needed = min(min_edges_per_node - G.degree(node), len(available_nodes))\n",
    "                    targets = random.sample(available_nodes, targets_needed)\n",
    "                    for target in targets:\n",
    "                        G.add_edge(node, target)\n",
    "        \n",
    "        return G\n",
    "    \n",
    "    def generate_edge_attributes(self, num_edges: int) -> List[int]:\n",
    "        \"\"\"\n",
    "        Gera atributos realistas para as arestas (0-255)\n",
    "        \"\"\"\n",
    "        edge_attrs = []\n",
    "        \n",
    "        for _ in range(num_edges):\n",
    "            link_quality = np.random.choice(['good', 'medium', 'poor'], p=[0.4, 0.4, 0.2])\n",
    "            \n",
    "            if link_quality == 'good':\n",
    "                attr = np.random.randint(200, 256)\n",
    "            elif link_quality == 'medium':\n",
    "                attr = np.random.randint(100, 200)\n",
    "            else:\n",
    "                attr = np.random.randint(0, 100)\n",
    "                \n",
    "            edge_attrs.append(attr)\n",
    "            \n",
    "        return edge_attrs\n",
    "    \n",
    "    def get_shortest_path(self, G: nx.Graph, source: int, target: int, \n",
    "                         edge_attrs: List[int], edge_index: List[List[int]]) -> List[int]:\n",
    "        \"\"\"\n",
    "        Calcula caminho mais curto considerando pesos das arestas\n",
    "        \"\"\"\n",
    "        weighted_G = nx.Graph()\n",
    "        weighted_G.add_nodes_from(G.nodes())\n",
    "        \n",
    "        for i, (u, v) in enumerate(zip(edge_index[0][::2], edge_index[1][::2])):\n",
    "            weight = 256 - edge_attrs[i*2]\n",
    "            weighted_G.add_edge(u, v, weight=weight)\n",
    "        \n",
    "        try:\n",
    "            path = nx.shortest_path(weighted_G, source, target, weight='weight')\n",
    "            return path\n",
    "        except nx.NetworkXNoPath:\n",
    "            if weighted_G.has_edge(source, target):\n",
    "                return [source, target]\n",
    "            return []\n",
    "    \n",
    "    def generate_balanced_samples_for_graph(self, graph_id: int, G: nx.Graph, \n",
    "                                          samples_per_graph: int = 50) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Gera amostras balanceadas para um único grafo\n",
    "        \"\"\"\n",
    "        samples = []\n",
    "        \n",
    "        # Converter para formato edge_index\n",
    "        edges = list(G.edges())\n",
    "        edge_index = [[], []]\n",
    "        for u, v in edges:\n",
    "            edge_index[0].extend([u, v])\n",
    "            edge_index[1].extend([v, u])\n",
    "        \n",
    "        edge_attrs = self.generate_edge_attributes(len(edge_index[0]))\n",
    "        \n",
    "        nodes = list(G.nodes())\n",
    "        num_pairs = max(3, samples_per_graph // 15)\n",
    "        \n",
    "        positive_samples = []\n",
    "        negative_samples = []\n",
    "        \n",
    "        for _ in range(num_pairs):\n",
    "            source, target = random.sample(nodes, 2)\n",
    "            optimal_path = self.get_shortest_path(G, source, target, edge_attrs, edge_index)\n",
    "            \n",
    "            if len(optimal_path) < 2:\n",
    "                continue\n",
    "                \n",
    "            for i in range(len(optimal_path) - 1):\n",
    "                current_node = optimal_path[i]\n",
    "                next_optimal = optimal_path[i + 1]\n",
    "                neighbors = list(G.neighbors(current_node))\n",
    "                \n",
    "                # Amostra positiva\n",
    "                positive_sample = {\n",
    "                    'graph_id': graph_id,\n",
    "                    'pair': [source, target],\n",
    "                    'path_len': len(optimal_path),\n",
    "                    'num_nodes': len(nodes),\n",
    "                    'edge_index': edge_index,\n",
    "                    'edge_attr': edge_attrs,\n",
    "                    'current_node': current_node,\n",
    "                    'target_node': target,\n",
    "                    'candidate_node': next_optimal,\n",
    "                    'labels': [1]\n",
    "                }\n",
    "                positive_samples.append(positive_sample)\n",
    "                \n",
    "                # Amostras negativas\n",
    "                wrong_neighbors = [n for n in neighbors if n != next_optimal]\n",
    "                for wrong_neighbor in wrong_neighbors:\n",
    "                    negative_sample = {\n",
    "                        'graph_id': graph_id,\n",
    "                        'pair': [source, target],\n",
    "                        'path_len': len(optimal_path),\n",
    "                        'num_nodes': len(nodes),\n",
    "                        'edge_index': edge_index,\n",
    "                        'edge_attr': edge_attrs,\n",
    "                        'current_node': current_node,\n",
    "                        'target_node': target,\n",
    "                        'candidate_node': wrong_neighbor,\n",
    "                        'labels': [0]\n",
    "                    }\n",
    "                    negative_samples.append(negative_sample)\n",
    "        \n",
    "        # Balanceamento\n",
    "        min_samples = min(len(positive_samples), len(negative_samples))\n",
    "        if min_samples == 0:\n",
    "            return []\n",
    "            \n",
    "        min_samples = max(min_samples, samples_per_graph // 4)\n",
    "        \n",
    "        balanced_positive = random.sample(positive_samples, min(len(positive_samples), min_samples))\n",
    "        balanced_negative = random.sample(negative_samples, min(len(negative_samples), min_samples))\n",
    "        \n",
    "        samples.extend(balanced_positive)\n",
    "        samples.extend(balanced_negative)\n",
    "        \n",
    "        return samples\n",
    "    \n",
    "    def generate_dataset(self, num_graphs: int = 100, \n",
    "                        node_range: Tuple[int, int] = (8, 20),\n",
    "                        samples_per_graph: int = 50) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Gera dataset completo com progress bar\n",
    "        \"\"\"\n",
    "        all_samples = []\n",
    "        \n",
    "        print(f\"Gerando {num_graphs} grafos...\")\n",
    "        \n",
    "        # Progress bar para notebooks\n",
    "        for graph_id in tqdm(range(num_graphs), desc=\"Gerando grafos\"):\n",
    "            num_nodes = np.random.randint(node_range[0], node_range[1] + 1)\n",
    "            edge_prob = max(0.2, min(0.6, 3.0 / num_nodes))\n",
    "            \n",
    "            G = self.generate_graph(num_nodes, edge_prob)\n",
    "            graph_samples = self.generate_balanced_samples_for_graph(\n",
    "                graph_id, G, samples_per_graph\n",
    "            )\n",
    "            all_samples.extend(graph_samples)\n",
    "        \n",
    "        random.shuffle(all_samples)\n",
    "        \n",
    "        # Estatísticas\n",
    "        positive_count = sum(1 for s in all_samples if s['labels'][0] == 1)\n",
    "        negative_count = len(all_samples) - positive_count\n",
    "        \n",
    "        print(f\"\\nDataset gerado com sucesso!\")\n",
    "        print(f\"Total de amostras: {len(all_samples):,}\")\n",
    "        print(f\"Amostras positivas: {positive_count:,} ({positive_count/len(all_samples)*100:.1f}%)\")\n",
    "        print(f\"Amostras negativas: {negative_count:,} ({negative_count/len(all_samples)*100:.1f}%)\")\n",
    "        print(f\"Balanceamento: {min(positive_count, negative_count) / max(positive_count, negative_count):.3f}\")\n",
    "        \n",
    "        return all_samples\n",
    "    \n",
    "    def generate_train_test_val_datasets(self, \n",
    "                                        total_graphs: int = 1000,\n",
    "                                        node_range: Tuple[int, int] = (8, 20),\n",
    "                                        samples_per_graph: int = 50,\n",
    "                                        train_ratio: float = 0.7,\n",
    "                                        test_ratio: float = 0.2,\n",
    "                                        val_ratio: float = 0.1,\n",
    "                                        output_dir: str = \"\"):\n",
    "        \"\"\"\n",
    "        Gera datasets de treino, teste e validação\n",
    "        \"\"\"\n",
    "        # Verificações\n",
    "        if abs(train_ratio + test_ratio + val_ratio - 1.0) > 1e-6:\n",
    "            raise ValueError(\"Os ratios devem somar 1.0!\")\n",
    "        \n",
    "        # Criar diretório\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"Gerando dataset completo com {total_graphs:,} grafos\")\n",
    "        print(f\"Diretório de saída: {output_dir}\")\n",
    "        print(f\"Splits: Train={train_ratio:.1%} | Test={test_ratio:.1%} | Val={val_ratio:.1%}\")\n",
    "        \n",
    "        # Gerar dataset completo\n",
    "        all_samples = self.generate_dataset(\n",
    "            num_graphs=total_graphs,\n",
    "            node_range=node_range,\n",
    "            samples_per_graph=samples_per_graph\n",
    "        )\n",
    "        \n",
    "        # Organizar por graph_id\n",
    "        samples_by_graph = defaultdict(list)\n",
    "        for sample in all_samples:\n",
    "            samples_by_graph[sample['graph_id']].append(sample)\n",
    "        \n",
    "        graph_ids = list(samples_by_graph.keys())\n",
    "        random.shuffle(graph_ids)\n",
    "        \n",
    "        # Calcular splits\n",
    "        num_train_graphs = int(len(graph_ids) * train_ratio)\n",
    "        num_test_graphs = int(len(graph_ids) * test_ratio)\n",
    "        num_val_graphs = len(graph_ids) - num_train_graphs - num_test_graphs\n",
    "        \n",
    "        train_graph_ids = graph_ids[:num_train_graphs]\n",
    "        test_graph_ids = graph_ids[num_train_graphs:num_train_graphs + num_test_graphs]\n",
    "        val_graph_ids = graph_ids[num_train_graphs + num_test_graphs:]\n",
    "        \n",
    "        # Criar datasets\n",
    "        train_samples = []\n",
    "        test_samples = []\n",
    "        val_samples = []\n",
    "        \n",
    "        for graph_id in train_graph_ids:\n",
    "            train_samples.extend(samples_by_graph[graph_id])\n",
    "        for graph_id in test_graph_ids:\n",
    "            test_samples.extend(samples_by_graph[graph_id])\n",
    "        for graph_id in val_graph_ids:\n",
    "            val_samples.extend(samples_by_graph[graph_id])\n",
    "        \n",
    "        # Shuffle\n",
    "        random.shuffle(train_samples)\n",
    "        random.shuffle(test_samples)\n",
    "        random.shuffle(val_samples)\n",
    "        \n",
    "        # Salvar com progress\n",
    "        datasets = [\n",
    "            (\"train\", train_samples),\n",
    "            (\"test\", test_samples), \n",
    "            (\"validation\", val_samples)\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nSalvando datasets...\")\n",
    "        \n",
    "        for name, samples in datasets:\n",
    "            filepath = os.path.join(output_dir, f\"{name}.jsonl\")\n",
    "            \n",
    "            with open(filepath, 'w') as f:\n",
    "                for sample in tqdm(samples, desc=f\"Salvando {name}\", leave=False):\n",
    "                    f.write(json.dumps(sample) + '\\n')\n",
    "            \n",
    "        # Relatório final\n",
    "        print(f\"DATASET\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        for name, samples in datasets:\n",
    "            if samples:\n",
    "                pos_samples = sum(1 for s in samples if s['labels'][0] == 1)\n",
    "                neg_samples = len(samples) - pos_samples\n",
    "                balance = min(pos_samples, neg_samples) / max(pos_samples, neg_samples)\n",
    "                \n",
    "                print(f\"\\n{name.upper()}:\")\n",
    "                print(f\"   Amostras: {len(samples):,}\")\n",
    "                print(f\"   Positivas: {pos_samples:,} ({pos_samples/len(samples)*100:.1f}%)\")\n",
    "                print(f\"   Negativas: {neg_samples:,} ({neg_samples/len(samples)*100:.1f}%)\")\n",
    "                print(f\"   Balanceamento: {balance:.3f}\")\n",
    "        \n",
    "        print(f\"\\nDatasets gerados com sucesso!\")\n",
    "        \n",
    "        return {\n",
    "            'train': train_samples,\n",
    "            'test': test_samples,\n",
    "            'validation': val_samples\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d1f10e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed definida como: 42\n"
     ]
    }
   ],
   "source": [
    "generator = RoutingDatasetGenerator(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f82599d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gerando dataset completo com 100 grafos\n",
      "Diretório de saída: ../datasets\n",
      "Splits: Train=70.0% | Test=20.0% | Val=10.0%\n",
      "Gerando 100 grafos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando grafos: 100%|██████████| 100/100 [00:00<00:00, 713.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset gerado com sucesso!\n",
      "Total de amostras: 1,656\n",
      "Amostras positivas: 828 (50.0%)\n",
      "Amostras negativas: 828 (50.0%)\n",
      "Balanceamento: 1.000\n",
      "\n",
      "Salvando datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET\n",
      "==================================================\n",
      "\n",
      "TRAIN:\n",
      "   Amostras: 1,164\n",
      "   Positivas: 582 (50.0%)\n",
      "   Negativas: 582 (50.0%)\n",
      "   Balanceamento: 1.000\n",
      "\n",
      "TEST:\n",
      "   Amostras: 318\n",
      "   Positivas: 159 (50.0%)\n",
      "   Negativas: 159 (50.0%)\n",
      "   Balanceamento: 1.000\n",
      "\n",
      "VALIDATION:\n",
      "   Amostras: 174\n",
      "   Positivas: 87 (50.0%)\n",
      "   Negativas: 87 (50.0%)\n",
      "   Balanceamento: 1.000\n",
      "\n",
      "Datasets gerados com sucesso!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Gerar os 3 datasets\n",
    "datasets = generator.generate_train_test_val_datasets(\n",
    "    total_graphs=100,          # Tamanho do Dataset\n",
    "    node_range=(8, 25),         # Tamanho dos grafos\n",
    "    samples_per_graph=10,       # Densidade\n",
    "    train_ratio=0.7,            # 70% treino\n",
    "    test_ratio=0.2,             # 20% teste\n",
    "    val_ratio=0.1,              # 10% validação\n",
    "    output_dir=\"../datasets\"    # Pasta local\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
